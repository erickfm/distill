"""
SFT + Re-invoke baseline (Lu & Lab, 2025).

Two-phase training:
1. Standard SFT on the task demonstrations
2. On-policy distillation from the base policy on general-purpose prompts
   to restore prior capabilities.
"""

from __future__ import annotations

import copy
import numpy as np
import torch
import torch.nn.functional as F
from tqdm.auto import tqdm
from torch.optim.lr_scheduler import LambdaLR
from typing import Optional

from src.run.config import RunConfig
from src.run.train.sft import do_sft
from src.run.utils import set_seeds, save_checkpoint
from src.run.logger import get_tqdm_kwargs
from src.run.distributed import is_main_process, barrier, get_raw_model


def do_sft_reinvoke(
    model: torch.nn.Module,
    config: RunConfig,
    reinvoke_prompts: Optional[list[str]] = None,
    reinvoke_steps: int = 200,
    reinvoke_lr: float = 1e-5,
    state: dict | None = None,
) -> tuple[torch.nn.Module, dict]:
    """
    SFT followed by on-policy distillation from the base policy.

    Phase 1: Standard SFT on task data.
    Phase 2: Generate from the base policy on general prompts,
             then distill those outputs back into the SFT'd model.

    Args:
        model: HFModel wrapping the causal LM
        config: Run configuration
        reinvoke_prompts: General-purpose prompts for phase 2
        reinvoke_steps: Number of re-invocation distillation steps
        reinvoke_lr: Learning rate for phase 2
        state: Optional state for resumption

    Returns:
        (trained_model, state_dict)
    """
    logger = config.logger

    # Phase 1: Standard SFT
    logger.info("---- SFT+Re-invoke: Phase 1 (SFT) ----")
    model, sft_state = do_sft(model, config, state)

    # Save the base (pre-SFT) model for distillation â€” we need it for phase 2
    # In practice, the base model should be stored before SFT.
    # Here we assume the frozen base is passed or we skip if no reinvoke prompts.
    if reinvoke_prompts is None or len(reinvoke_prompts) == 0:
        logger.warning("No reinvoke prompts provided; skipping phase 2")
        return model, sft_state

    # Phase 2: On-policy distillation from base policy
    logger.info(f"---- SFT+Re-invoke: Phase 2 (Re-invoke, {reinvoke_steps} steps) ----")

    raw_model = get_raw_model(model)
    device = config.device

    opt = torch.optim.AdamW(model.parameters(), lr=reinvoke_lr, weight_decay=0.01)

    model.train()

    pbar = tqdm(total=reinvoke_steps, **get_tqdm_kwargs(logger, desc="Re-invoke", ncols=100))

    for step in range(reinvoke_steps):
        # Sample a random general prompt
        prompt_idx = step % len(reinvoke_prompts)
        prompt = reinvoke_prompts[prompt_idx]

        # Tokenize prompt
        enc = raw_model.tokenizer(prompt, return_tensors="pt").to(device)

        # Generate from the current model
        with torch.no_grad():
            generated = raw_model.generate(
                input_ids=enc.input_ids,
                attention_mask=enc.attention_mask,
                max_new_tokens=512,
                temperature=1.0,
                do_sample=True,
            )

        # Train on the generated output (self-distillation from base)
        gen_mask = torch.ones_like(generated)
        outputs = raw_model.model(input_ids=generated, attention_mask=gen_mask)
        logits = outputs.logits[:, :-1, :]
        targets = generated[:, 1:]

        loss = F.cross_entropy(
            logits.reshape(-1, logits.size(-1)),
            targets.reshape(-1),
            ignore_index=raw_model.tokenizer.pad_token_id,
        )

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        opt.step()
        opt.zero_grad(set_to_none=True)

        pbar.update()
        if step % 50 == 0:
            logger.info(f"Re-invoke step {step}/{reinvoke_steps} | Loss: {loss.item():.4f}")

    pbar.close()

    state = {**sft_state, "reinvoke_steps": reinvoke_steps}
    return model, state

